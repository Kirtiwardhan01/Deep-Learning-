{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "authorship_tag": "ABX9TyOGR5uM+mb0wXVwzlc57XZr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kirtiwardhan01/Deep-Learning-/blob/master/Supervised%20Regression%20Problem%20using%20tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5arul_nzvClB",
        "colab_type": "text"
      },
      "source": [
        "##**Bike Sharing Dataset Data Set**\n",
        "\n",
        "## In this assignment we are going to predict demand of bikes in Washington D.C "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjKXiAOlvjqp",
        "colab_type": "text"
      },
      "source": [
        "**Import the data file for local repository using the command below**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odArCH5ZvtsY",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "f47c54e7-d049-48d8-9c5f-cacd51fe2bb9"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f8a99f20-dfc9-49c4-b374-7f93e07dd1c9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f8a99f20-dfc9-49c4-b374-7f93e07dd1c9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUkqVYjovB0c",
        "colab_type": "text"
      },
      "source": [
        "Attribute Information:\n",
        "\n",
        "Both hour.csv and day.csv have the following fields, except hr which is not available in day.csv\n",
        "\n",
        "- instant: record index\n",
        "- dteday : date\n",
        "- season : season (1:winter, 2:spring, 3:summer, 4:fall)\n",
        "- yr : year (0: 2011, 1:2012)\n",
        "- mnth : month ( 1 to 12)\n",
        "- hr : hour (0 to 23)\n",
        "- holiday : weather day is holiday or not (extracted from [Web Link])\n",
        "- weekday : day of the week\n",
        "- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
        "+ weathersit :\n",
        "- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
        "- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
        "- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
        "- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
        "- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n",
        "- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n",
        "- hum: Normalized humidity. The values are divided to 100 (max)\n",
        "- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
        "- casual: count of casual users\n",
        "- registered: count of registered users\n",
        "- cnt: count of total rental bikes including both casual and registered"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gqxg3iQrOBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as numpy\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "pd.options.display.max_columns=None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mScKQ5xTr-ja",
        "colab_type": "code",
        "outputId": "a537f34c-9fb4-438b-c398-9c3473c1e261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "bike_df = pd.read_csv('/content/attachment_attachment_hour_lyst8188_lyst2724.csv')\n",
        "bike_df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
              "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
              "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
              "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
              "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
              "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
              "\n",
              "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
              "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
              "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
              "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
              "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
              "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqUOTgOSv4wp",
        "colab_type": "text"
      },
      "source": [
        "### Dummy encode the categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pCPfHwEsVIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "season_dummies=pd.get_dummies(bike_df.season, prefix='season',drop_first=True)\n",
        "mnth_dummies=pd.get_dummies(bike_df.mnth, prefix='month',drop_first=True)\n",
        "weather_dummies=pd.get_dummies(bike_df.weathersit,prefix='weather',drop_first=True)\n",
        "weekday_dummies=pd.get_dummies(bike_df.weekday,prefix='weekday',drop_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4px5gF7v9sa",
        "colab_type": "text"
      },
      "source": [
        "**Append the dummy-encoded variables with original dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNishl2UsuJe",
        "colab_type": "code",
        "outputId": "ae1dd663-d161-4872-8f63-22b8958604b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bike_df = pd.concat([bike_df,season_dummies,mnth_dummies,weather_dummies,weekday_dummies],axis=1)\n",
        "bike_df.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17379, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cixjppG-wBan",
        "colab_type": "text"
      },
      "source": [
        "**Drop the variables which were dummy-encoded**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM7veVZQuofn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del bike_df['season']\n",
        "del bike_df['mnth']\n",
        "del bike_df['weekday']\n",
        "del bike_df['weathersit']\n",
        "del bike_df['instant']\n",
        "del bike_df['dteday']\n",
        "del bike_df['casual']\n",
        "del bike_df['registered']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUKL5J1_v6u1",
        "colab_type": "code",
        "outputId": "a2986913-12ca-4028-c9d7-cf5242183ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bike_df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17379, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oexCpbFvv_EK",
        "colab_type": "code",
        "outputId": "dff945a8-2a3e-4432-84c7-375f22705e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "bike_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>yr</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>cnt</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>month_2</th>\n",
              "      <th>month_3</th>\n",
              "      <th>month_4</th>\n",
              "      <th>month_5</th>\n",
              "      <th>month_6</th>\n",
              "      <th>month_7</th>\n",
              "      <th>month_8</th>\n",
              "      <th>month_9</th>\n",
              "      <th>month_10</th>\n",
              "      <th>month_11</th>\n",
              "      <th>month_12</th>\n",
              "      <th>weather_2</th>\n",
              "      <th>weather_3</th>\n",
              "      <th>weather_4</th>\n",
              "      <th>weekday_1</th>\n",
              "      <th>weekday_2</th>\n",
              "      <th>weekday_3</th>\n",
              "      <th>weekday_4</th>\n",
              "      <th>weekday_5</th>\n",
              "      <th>weekday_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   yr  hr  holiday  workingday  temp   atemp   hum  windspeed  cnt  season_2  \\\n",
              "0   0   0        0           0  0.24  0.2879  0.81        0.0   16         0   \n",
              "1   0   1        0           0  0.22  0.2727  0.80        0.0   40         0   \n",
              "2   0   2        0           0  0.22  0.2727  0.80        0.0   32         0   \n",
              "3   0   3        0           0  0.24  0.2879  0.75        0.0   13         0   \n",
              "4   0   4        0           0  0.24  0.2879  0.75        0.0    1         0   \n",
              "\n",
              "   season_3  season_4  month_2  month_3  month_4  month_5  month_6  month_7  \\\n",
              "0         0         0        0        0        0        0        0        0   \n",
              "1         0         0        0        0        0        0        0        0   \n",
              "2         0         0        0        0        0        0        0        0   \n",
              "3         0         0        0        0        0        0        0        0   \n",
              "4         0         0        0        0        0        0        0        0   \n",
              "\n",
              "   month_8  month_9  month_10  month_11  month_12  weather_2  weather_3  \\\n",
              "0        0        0         0         0         0          0          0   \n",
              "1        0        0         0         0         0          0          0   \n",
              "2        0        0         0         0         0          0          0   \n",
              "3        0        0         0         0         0          0          0   \n",
              "4        0        0         0         0         0          0          0   \n",
              "\n",
              "   weather_4  weekday_1  weekday_2  weekday_3  weekday_4  weekday_5  weekday_6  \n",
              "0          0          0          0          0          0          0          1  \n",
              "1          0          0          0          0          0          0          1  \n",
              "2          0          0          0          0          0          0          1  \n",
              "3          0          0          0          0          0          0          1  \n",
              "4          0          0          0          0          0          0          1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I10nSlDAwKVu",
        "colab_type": "text"
      },
      "source": [
        "**Convert the target variable into float64 data type and define input features and output variable**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVRYRzBGwGn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's convert dtype of cnt to float\n",
        "bike_df['cnt'] = bike_df['cnt'].astype('float64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0DcRK__wowo",
        "colab_type": "code",
        "outputId": "f0f97ffb-5571-49e8-cf60-db828973a843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "target = bike_df['cnt']\n",
        "bike_df.drop(['cnt'],axis=1,inplace=True)\n",
        "features = bike_df\n",
        "print(features.shape)\n",
        "print(target.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17379, 31)\n",
            "(17379,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt9cn9Hqxlxz",
        "colab_type": "code",
        "outputId": "f0fc1d42-ee09-41af-8011-e933a6e02aee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "features.describe()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>yr</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>month_2</th>\n",
              "      <th>month_3</th>\n",
              "      <th>month_4</th>\n",
              "      <th>month_5</th>\n",
              "      <th>month_6</th>\n",
              "      <th>month_7</th>\n",
              "      <th>month_8</th>\n",
              "      <th>month_9</th>\n",
              "      <th>month_10</th>\n",
              "      <th>month_11</th>\n",
              "      <th>month_12</th>\n",
              "      <th>weather_2</th>\n",
              "      <th>weather_3</th>\n",
              "      <th>weather_4</th>\n",
              "      <th>weekday_1</th>\n",
              "      <th>weekday_2</th>\n",
              "      <th>weekday_3</th>\n",
              "      <th>weekday_4</th>\n",
              "      <th>weekday_5</th>\n",
              "      <th>weekday_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.502561</td>\n",
              "      <td>11.546752</td>\n",
              "      <td>0.028770</td>\n",
              "      <td>0.682721</td>\n",
              "      <td>0.496987</td>\n",
              "      <td>0.475775</td>\n",
              "      <td>0.627229</td>\n",
              "      <td>0.190098</td>\n",
              "      <td>0.253697</td>\n",
              "      <td>0.258703</td>\n",
              "      <td>0.243512</td>\n",
              "      <td>0.077162</td>\n",
              "      <td>0.084757</td>\n",
              "      <td>0.082686</td>\n",
              "      <td>0.085621</td>\n",
              "      <td>0.082859</td>\n",
              "      <td>0.085621</td>\n",
              "      <td>0.084873</td>\n",
              "      <td>0.082686</td>\n",
              "      <td>0.083492</td>\n",
              "      <td>0.082686</td>\n",
              "      <td>0.085333</td>\n",
              "      <td>0.261465</td>\n",
              "      <td>0.081650</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.142643</td>\n",
              "      <td>0.141147</td>\n",
              "      <td>0.142413</td>\n",
              "      <td>0.142183</td>\n",
              "      <td>0.143104</td>\n",
              "      <td>0.144542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.500008</td>\n",
              "      <td>6.914405</td>\n",
              "      <td>0.167165</td>\n",
              "      <td>0.465431</td>\n",
              "      <td>0.192556</td>\n",
              "      <td>0.171850</td>\n",
              "      <td>0.192930</td>\n",
              "      <td>0.122340</td>\n",
              "      <td>0.435139</td>\n",
              "      <td>0.437935</td>\n",
              "      <td>0.429214</td>\n",
              "      <td>0.266856</td>\n",
              "      <td>0.278528</td>\n",
              "      <td>0.275415</td>\n",
              "      <td>0.279811</td>\n",
              "      <td>0.275676</td>\n",
              "      <td>0.279811</td>\n",
              "      <td>0.278700</td>\n",
              "      <td>0.275415</td>\n",
              "      <td>0.276632</td>\n",
              "      <td>0.275415</td>\n",
              "      <td>0.279384</td>\n",
              "      <td>0.439445</td>\n",
              "      <td>0.273839</td>\n",
              "      <td>0.013138</td>\n",
              "      <td>0.349719</td>\n",
              "      <td>0.348184</td>\n",
              "      <td>0.349484</td>\n",
              "      <td>0.349248</td>\n",
              "      <td>0.350189</td>\n",
              "      <td>0.351649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.333300</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.104500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.484800</td>\n",
              "      <td>0.630000</td>\n",
              "      <td>0.194000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.621200</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.253700</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.850700</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 yr            hr       holiday    workingday          temp  \\\n",
              "count  17379.000000  17379.000000  17379.000000  17379.000000  17379.000000   \n",
              "mean       0.502561     11.546752      0.028770      0.682721      0.496987   \n",
              "std        0.500008      6.914405      0.167165      0.465431      0.192556   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.020000   \n",
              "25%        0.000000      6.000000      0.000000      0.000000      0.340000   \n",
              "50%        1.000000     12.000000      0.000000      1.000000      0.500000   \n",
              "75%        1.000000     18.000000      0.000000      1.000000      0.660000   \n",
              "max        1.000000     23.000000      1.000000      1.000000      1.000000   \n",
              "\n",
              "              atemp           hum     windspeed      season_2      season_3  \\\n",
              "count  17379.000000  17379.000000  17379.000000  17379.000000  17379.000000   \n",
              "mean       0.475775      0.627229      0.190098      0.253697      0.258703   \n",
              "std        0.171850      0.192930      0.122340      0.435139      0.437935   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "25%        0.333300      0.480000      0.104500      0.000000      0.000000   \n",
              "50%        0.484800      0.630000      0.194000      0.000000      0.000000   \n",
              "75%        0.621200      0.780000      0.253700      1.000000      1.000000   \n",
              "max        1.000000      1.000000      0.850700      1.000000      1.000000   \n",
              "\n",
              "           season_4       month_2       month_3       month_4       month_5  \\\n",
              "count  17379.000000  17379.000000  17379.000000  17379.000000  17379.000000   \n",
              "mean       0.243512      0.077162      0.084757      0.082686      0.085621   \n",
              "std        0.429214      0.266856      0.278528      0.275415      0.279811   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
              "\n",
              "            month_6       month_7       month_8       month_9      month_10  \\\n",
              "count  17379.000000  17379.000000  17379.000000  17379.000000  17379.000000   \n",
              "mean       0.082859      0.085621      0.084873      0.082686      0.083492   \n",
              "std        0.275676      0.279811      0.278700      0.275415      0.276632   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
              "\n",
              "           month_11      month_12     weather_2     weather_3     weather_4  \\\n",
              "count  17379.000000  17379.000000  17379.000000  17379.000000  17379.000000   \n",
              "mean       0.082686      0.085333      0.261465      0.081650      0.000173   \n",
              "std        0.275415      0.279384      0.439445      0.273839      0.013138   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "75%        0.000000      0.000000      1.000000      0.000000      0.000000   \n",
              "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
              "\n",
              "          weekday_1     weekday_2     weekday_3     weekday_4     weekday_5  \\\n",
              "count  17379.000000  17379.000000  17379.000000  17379.000000  17379.000000   \n",
              "mean       0.142643      0.141147      0.142413      0.142183      0.143104   \n",
              "std        0.349719      0.348184      0.349484      0.349248      0.350189   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
              "\n",
              "          weekday_6  \n",
              "count  17379.000000  \n",
              "mean       0.144542  \n",
              "std        0.351649  \n",
              "min        0.000000  \n",
              "25%        0.000000  \n",
              "50%        0.000000  \n",
              "75%        0.000000  \n",
              "max        1.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJMg1gGUwREn",
        "colab_type": "text"
      },
      "source": [
        "### Input features are given in different scale and should be stadardized before feeding them into Neural Network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHbONTZyxu88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FTJ5DVsyfAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_ss = ss.fit_transform(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8-QmAD_wW-8",
        "colab_type": "text"
      },
      "source": [
        "**Split input and output into desired splits of train and validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWvUzO2Gy3ML",
        "colab_type": "code",
        "outputId": "88b5113c-c689-4da4-8d55-8fda360cb0e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(features_ss,target,test_size=0.2,random_state=12)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13903, 31)\n",
            "(3476, 31)\n",
            "(13903,)\n",
            "(3476,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHt_jWUawbw9",
        "colab_type": "text"
      },
      "source": [
        "## Build model\n",
        "**Import the libraries from keras to model our data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnfgZhYCzfuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbG5_8W-0sHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ugtXMmf1H35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(150,activation='relu',input_shape=(31,)))      ## input shape 31 i.e we have 31 features and keep 150 neurons in the 1st hidden layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oYyL1Yo1cNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(90,activation='relu'))\n",
        "model.add(Dense(60,activation='relu'))\n",
        "model.add(Dense(30,activation='relu'))\n",
        "model.add(Dense(1,activation='relu'))       ## We solving regression problem and should get output from one neuron\n",
        "model.compile(optimizer='adam',loss='mse')  # We'll keep 'mse' as evaluation of the model for the time being later we'll find\n",
        "                                                                                 # RMSE of the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg2eUcPv2ODf",
        "colab_type": "code",
        "outputId": "439213ab-3d47-4ae7-f493-33cd79ab0f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model.summary()    # Use summary to find parameters used across different layers of the NN model"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 150)               4800      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 90)                13590     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 60)                5460      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 30)                1830      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 25,711\n",
            "Trainable params: 25,711\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tudnHR7xIKW",
        "colab_type": "text"
      },
      "source": [
        "**We need to have tensorboard see how parameters have been distributed across the layers and how optimizer has behaved at different epochs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-cXXhen2dOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It saves the best model having the least loss\n",
        "filepath = 'weight.bike.preprocess.best.hdf5'\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath,monitor='loss',verbose=1,save_best_only=True,mode='auto')\n",
        "log_dir = './tf-log/bike_v4'\n",
        "tb_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9uwz_1p3a4J",
        "colab_type": "code",
        "outputId": "71d7fea2-dcc5-4f45-fc04-995139394999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!wget -q https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yymqKDW4qgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(log_dir)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpWOz-1O5Q2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_zAJZOF5p9b",
        "colab_type": "code",
        "outputId": "6186e89e-e2e5-47da-ddd5-98a29b95653b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://ed9e0d4a.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbuRVdXkxUPt",
        "colab_type": "text"
      },
      "source": [
        "**Click on the link above to find the tensorboard of your Neural Network Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONyTn1B15vrX",
        "colab_type": "code",
        "outputId": "d0f27149-ca1c-4c49-d042-d8641c6c2b76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''from keras.callbacks import EarlyStopping\n",
        "stop=EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=1, mode='auto')'''"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from keras.callbacks import EarlyStopping\\nstop=EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=1, mode='auto')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TLLh9HRxZOw",
        "colab_type": "text"
      },
      "source": [
        "## Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEqUDP7P6Ok2",
        "colab_type": "code",
        "outputId": "cf9bec1b-0b7b-4f08-eee5-fad45e47b853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train,Y_train,epochs=100,batch_size=32,callbacks=[tb_cb,checkpoint])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "427/435 [============================>.] - ETA: 0s - loss: 25251.1152\n",
            "Epoch 00001: loss improved from inf to 25096.20898, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 25096.2090\n",
            "Epoch 2/100\n",
            "426/435 [============================>.] - ETA: 0s - loss: 18413.3027\n",
            "Epoch 00002: loss improved from 25096.20898 to 18337.32031, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 18337.3203\n",
            "Epoch 3/100\n",
            "427/435 [============================>.] - ETA: 0s - loss: 15367.4023\n",
            "Epoch 00003: loss improved from 18337.32031 to 15318.35938, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 15318.3594\n",
            "Epoch 4/100\n",
            "431/435 [============================>.] - ETA: 0s - loss: 11298.6260\n",
            "Epoch 00004: loss improved from 15318.35938 to 11242.10449, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 11242.1045\n",
            "Epoch 5/100\n",
            "425/435 [============================>.] - ETA: 0s - loss: 9537.9697\n",
            "Epoch 00005: loss improved from 11242.10449 to 9519.27246, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 9519.2725\n",
            "Epoch 6/100\n",
            "421/435 [============================>.] - ETA: 0s - loss: 8133.8486\n",
            "Epoch 00006: loss improved from 9519.27246 to 8096.86328, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 8096.8633\n",
            "Epoch 7/100\n",
            "425/435 [============================>.] - ETA: 0s - loss: 6629.2637\n",
            "Epoch 00007: loss improved from 8096.86328 to 6605.11377, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 6605.1138\n",
            "Epoch 8/100\n",
            "425/435 [============================>.] - ETA: 0s - loss: 5281.7329\n",
            "Epoch 00008: loss improved from 6605.11377 to 5274.40332, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 5274.4033\n",
            "Epoch 9/100\n",
            "431/435 [============================>.] - ETA: 0s - loss: 4253.9019\n",
            "Epoch 00009: loss improved from 5274.40332 to 4250.24902, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 4250.2490\n",
            "Epoch 10/100\n",
            "416/435 [===========================>..] - ETA: 0s - loss: 3492.8694\n",
            "Epoch 00010: loss improved from 4250.24902 to 3505.96094, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 3505.9609\n",
            "Epoch 11/100\n",
            "428/435 [============================>.] - ETA: 0s - loss: 3038.7549\n",
            "Epoch 00011: loss improved from 3505.96094 to 3019.60010, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 3019.6001\n",
            "Epoch 12/100\n",
            "424/435 [============================>.] - ETA: 0s - loss: 2640.4563\n",
            "Epoch 00012: loss improved from 3019.60010 to 2647.73413, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 2647.7341\n",
            "Epoch 13/100\n",
            "428/435 [============================>.] - ETA: 0s - loss: 2355.5903\n",
            "Epoch 00013: loss improved from 2647.73413 to 2349.55127, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 2349.5513\n",
            "Epoch 14/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 2220.3311\n",
            "Epoch 00014: loss improved from 2349.55127 to 2215.89990, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 2215.8999\n",
            "Epoch 15/100\n",
            "432/435 [============================>.] - ETA: 0s - loss: 2050.6746\n",
            "Epoch 00015: loss improved from 2215.89990 to 2052.31421, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 2052.3142\n",
            "Epoch 16/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 1897.0332\n",
            "Epoch 00016: loss improved from 2052.31421 to 1900.52246, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1900.5225\n",
            "Epoch 17/100\n",
            "424/435 [============================>.] - ETA: 0s - loss: 1848.1340\n",
            "Epoch 00017: loss improved from 1900.52246 to 1842.98047, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1842.9805\n",
            "Epoch 18/100\n",
            "428/435 [============================>.] - ETA: 0s - loss: 1819.3964\n",
            "Epoch 00018: loss improved from 1842.98047 to 1814.05188, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1814.0519\n",
            "Epoch 19/100\n",
            "435/435 [==============================] - ETA: 0s - loss: 1775.3090\n",
            "Epoch 00019: loss improved from 1814.05188 to 1775.30896, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1775.3090\n",
            "Epoch 20/100\n",
            "418/435 [===========================>..] - ETA: 0s - loss: 1633.4540\n",
            "Epoch 00020: loss improved from 1775.30896 to 1637.82556, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1637.8256\n",
            "Epoch 21/100\n",
            "435/435 [==============================] - ETA: 0s - loss: 1572.6969\n",
            "Epoch 00021: loss improved from 1637.82556 to 1572.69690, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1572.6969\n",
            "Epoch 22/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 1555.8434\n",
            "Epoch 00022: loss improved from 1572.69690 to 1556.70593, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1556.7059\n",
            "Epoch 23/100\n",
            "420/435 [===========================>..] - ETA: 0s - loss: 1492.8402\n",
            "Epoch 00023: loss improved from 1556.70593 to 1516.35095, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1516.3510\n",
            "Epoch 24/100\n",
            "432/435 [============================>.] - ETA: 0s - loss: 1462.7899\n",
            "Epoch 00024: loss improved from 1516.35095 to 1459.14026, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1459.1403\n",
            "Epoch 25/100\n",
            "434/435 [============================>.] - ETA: 0s - loss: 1445.7338\n",
            "Epoch 00025: loss improved from 1459.14026 to 1444.12488, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1444.1249\n",
            "Epoch 26/100\n",
            "430/435 [============================>.] - ETA: 0s - loss: 1431.0312\n",
            "Epoch 00026: loss improved from 1444.12488 to 1430.90161, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1430.9016\n",
            "Epoch 27/100\n",
            "429/435 [============================>.] - ETA: 0s - loss: 1353.9265\n",
            "Epoch 00027: loss improved from 1430.90161 to 1356.85486, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1356.8549\n",
            "Epoch 28/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 1325.9510\n",
            "Epoch 00028: loss improved from 1356.85486 to 1329.59436, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1329.5944\n",
            "Epoch 29/100\n",
            "434/435 [============================>.] - ETA: 0s - loss: 1358.7675\n",
            "Epoch 00029: loss did not improve from 1329.59436\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1357.2544\n",
            "Epoch 30/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 1319.2737\n",
            "Epoch 00030: loss improved from 1329.59436 to 1320.03906, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1320.0391\n",
            "Epoch 31/100\n",
            "421/435 [============================>.] - ETA: 0s - loss: 1281.0092\n",
            "Epoch 00031: loss improved from 1320.03906 to 1271.81921, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1271.8192\n",
            "Epoch 32/100\n",
            "435/435 [==============================] - ETA: 0s - loss: 1378.8929\n",
            "Epoch 00032: loss did not improve from 1271.81921\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1378.8929\n",
            "Epoch 33/100\n",
            "423/435 [============================>.] - ETA: 0s - loss: 1269.0588\n",
            "Epoch 00033: loss improved from 1271.81921 to 1266.65308, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1266.6531\n",
            "Epoch 34/100\n",
            "422/435 [============================>.] - ETA: 0s - loss: 1175.9554\n",
            "Epoch 00034: loss improved from 1266.65308 to 1174.45007, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1174.4501\n",
            "Epoch 35/100\n",
            "424/435 [============================>.] - ETA: 0s - loss: 1276.3373\n",
            "Epoch 00035: loss did not improve from 1174.45007\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1270.3319\n",
            "Epoch 36/100\n",
            "427/435 [============================>.] - ETA: 0s - loss: 1150.1587\n",
            "Epoch 00036: loss improved from 1174.45007 to 1146.54858, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1146.5486\n",
            "Epoch 37/100\n",
            "420/435 [===========================>..] - ETA: 0s - loss: 1127.4600\n",
            "Epoch 00037: loss improved from 1146.54858 to 1119.01196, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1119.0120\n",
            "Epoch 38/100\n",
            "424/435 [============================>.] - ETA: 0s - loss: 1122.7513\n",
            "Epoch 00038: loss improved from 1119.01196 to 1117.15515, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1117.1552\n",
            "Epoch 39/100\n",
            "418/435 [===========================>..] - ETA: 0s - loss: 1168.0391\n",
            "Epoch 00039: loss did not improve from 1117.15515\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1172.6864\n",
            "Epoch 40/100\n",
            "434/435 [============================>.] - ETA: 0s - loss: 1153.4690\n",
            "Epoch 00040: loss did not improve from 1117.15515\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1153.9348\n",
            "Epoch 41/100\n",
            "425/435 [============================>.] - ETA: 0s - loss: 1088.3529\n",
            "Epoch 00041: loss improved from 1117.15515 to 1088.85034, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1088.8503\n",
            "Epoch 42/100\n",
            "435/435 [==============================] - ETA: 0s - loss: 1045.0830\n",
            "Epoch 00042: loss improved from 1088.85034 to 1045.08301, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1045.0830\n",
            "Epoch 43/100\n",
            "430/435 [============================>.] - ETA: 0s - loss: 1075.8682\n",
            "Epoch 00043: loss did not improve from 1045.08301\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1076.0732\n",
            "Epoch 44/100\n",
            "429/435 [============================>.] - ETA: 0s - loss: 1052.3197\n",
            "Epoch 00044: loss did not improve from 1045.08301\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1053.2715\n",
            "Epoch 45/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 1021.7303\n",
            "Epoch 00045: loss improved from 1045.08301 to 1021.01855, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1021.0186\n",
            "Epoch 46/100\n",
            "435/435 [==============================] - ETA: 0s - loss: 1006.3541\n",
            "Epoch 00046: loss improved from 1021.01855 to 1006.35406, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1006.3541\n",
            "Epoch 47/100\n",
            "434/435 [============================>.] - ETA: 0s - loss: 1034.4336\n",
            "Epoch 00047: loss did not improve from 1006.35406\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1033.1715\n",
            "Epoch 48/100\n",
            "425/435 [============================>.] - ETA: 0s - loss: 971.1761\n",
            "Epoch 00048: loss improved from 1006.35406 to 970.59259, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 970.5926\n",
            "Epoch 49/100\n",
            "421/435 [============================>.] - ETA: 0s - loss: 938.7480\n",
            "Epoch 00049: loss improved from 970.59259 to 939.64069, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 939.6407\n",
            "Epoch 50/100\n",
            "431/435 [============================>.] - ETA: 0s - loss: 957.5019\n",
            "Epoch 00050: loss did not improve from 939.64069\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 954.7012\n",
            "Epoch 51/100\n",
            "429/435 [============================>.] - ETA: 0s - loss: 1029.5179\n",
            "Epoch 00051: loss did not improve from 939.64069\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1032.3257\n",
            "Epoch 52/100\n",
            "420/435 [===========================>..] - ETA: 0s - loss: 1000.0315\n",
            "Epoch 00052: loss did not improve from 939.64069\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 1009.6405\n",
            "Epoch 53/100\n",
            "428/435 [============================>.] - ETA: 0s - loss: 967.6709\n",
            "Epoch 00053: loss did not improve from 939.64069\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 970.4714\n",
            "Epoch 54/100\n",
            "420/435 [===========================>..] - ETA: 0s - loss: 938.4541\n",
            "Epoch 00054: loss improved from 939.64069 to 933.24091, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 933.2409\n",
            "Epoch 55/100\n",
            "422/435 [============================>.] - ETA: 0s - loss: 914.0208\n",
            "Epoch 00055: loss improved from 933.24091 to 914.00653, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 914.0065\n",
            "Epoch 56/100\n",
            "434/435 [============================>.] - ETA: 0s - loss: 923.8370\n",
            "Epoch 00056: loss did not improve from 914.00653\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 924.1356\n",
            "Epoch 57/100\n",
            "429/435 [============================>.] - ETA: 0s - loss: 905.5779\n",
            "Epoch 00057: loss improved from 914.00653 to 909.27228, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 909.2723\n",
            "Epoch 58/100\n",
            "417/435 [===========================>..] - ETA: 0s - loss: 843.3391\n",
            "Epoch 00058: loss improved from 909.27228 to 847.79504, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 847.7950\n",
            "Epoch 59/100\n",
            "422/435 [============================>.] - ETA: 0s - loss: 899.7073\n",
            "Epoch 00059: loss did not improve from 847.79504\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 906.5461\n",
            "Epoch 60/100\n",
            "432/435 [============================>.] - ETA: 0s - loss: 920.3616\n",
            "Epoch 00060: loss did not improve from 847.79504\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 921.4208\n",
            "Epoch 61/100\n",
            "430/435 [============================>.] - ETA: 0s - loss: 892.8459\n",
            "Epoch 00061: loss did not improve from 847.79504\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 890.8474\n",
            "Epoch 62/100\n",
            "429/435 [============================>.] - ETA: 0s - loss: 856.9781\n",
            "Epoch 00062: loss did not improve from 847.79504\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 854.9995\n",
            "Epoch 63/100\n",
            "423/435 [============================>.] - ETA: 0s - loss: 816.0677\n",
            "Epoch 00063: loss improved from 847.79504 to 812.12427, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 812.1243\n",
            "Epoch 64/100\n",
            "432/435 [============================>.] - ETA: 0s - loss: 872.2524\n",
            "Epoch 00064: loss did not improve from 812.12427\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 869.9847\n",
            "Epoch 65/100\n",
            "422/435 [============================>.] - ETA: 0s - loss: 835.1678\n",
            "Epoch 00065: loss did not improve from 812.12427\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 832.3389\n",
            "Epoch 66/100\n",
            "424/435 [============================>.] - ETA: 0s - loss: 823.3345\n",
            "Epoch 00066: loss did not improve from 812.12427\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 824.2712\n",
            "Epoch 67/100\n",
            "424/435 [============================>.] - ETA: 0s - loss: 849.4059\n",
            "Epoch 00067: loss did not improve from 812.12427\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 845.3184\n",
            "Epoch 68/100\n",
            "427/435 [============================>.] - ETA: 0s - loss: 744.7581\n",
            "Epoch 00068: loss improved from 812.12427 to 753.52655, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 753.5266\n",
            "Epoch 69/100\n",
            "426/435 [============================>.] - ETA: 0s - loss: 782.2321\n",
            "Epoch 00069: loss did not improve from 753.52655\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 781.5126\n",
            "Epoch 70/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 777.6132\n",
            "Epoch 00070: loss did not improve from 753.52655\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 779.7101\n",
            "Epoch 71/100\n",
            "435/435 [==============================] - ETA: 0s - loss: 797.3769\n",
            "Epoch 00071: loss did not improve from 753.52655\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 797.3769\n",
            "Epoch 72/100\n",
            "429/435 [============================>.] - ETA: 0s - loss: 789.0618\n",
            "Epoch 00072: loss did not improve from 753.52655\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 785.9303\n",
            "Epoch 73/100\n",
            "418/435 [===========================>..] - ETA: 0s - loss: 725.4498\n",
            "Epoch 00073: loss improved from 753.52655 to 731.82965, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 731.8297\n",
            "Epoch 74/100\n",
            "419/435 [===========================>..] - ETA: 0s - loss: 734.2252\n",
            "Epoch 00074: loss did not improve from 731.82965\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 753.1606\n",
            "Epoch 75/100\n",
            "430/435 [============================>.] - ETA: 0s - loss: 746.5778\n",
            "Epoch 00075: loss did not improve from 731.82965\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 744.5775\n",
            "Epoch 76/100\n",
            "427/435 [============================>.] - ETA: 0s - loss: 724.2161\n",
            "Epoch 00076: loss improved from 731.82965 to 729.40448, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 729.4045\n",
            "Epoch 77/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 768.4740\n",
            "Epoch 00077: loss did not improve from 729.40448\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 768.8644\n",
            "Epoch 78/100\n",
            "418/435 [===========================>..] - ETA: 0s - loss: 700.9169\n",
            "Epoch 00078: loss improved from 729.40448 to 706.53223, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 706.5322\n",
            "Epoch 79/100\n",
            "430/435 [============================>.] - ETA: 0s - loss: 721.1443\n",
            "Epoch 00079: loss did not improve from 706.53223\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 724.2772\n",
            "Epoch 80/100\n",
            "431/435 [============================>.] - ETA: 0s - loss: 720.5956\n",
            "Epoch 00080: loss did not improve from 706.53223\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 719.3790\n",
            "Epoch 81/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 725.1263\n",
            "Epoch 00081: loss did not improve from 706.53223\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 724.2232\n",
            "Epoch 82/100\n",
            "428/435 [============================>.] - ETA: 0s - loss: 696.3187\n",
            "Epoch 00082: loss improved from 706.53223 to 696.93262, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 696.9326\n",
            "Epoch 83/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 729.2448\n",
            "Epoch 00083: loss did not improve from 696.93262\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 729.2801\n",
            "Epoch 84/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 730.2584\n",
            "Epoch 00084: loss did not improve from 696.93262\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 730.1338\n",
            "Epoch 85/100\n",
            "423/435 [============================>.] - ETA: 0s - loss: 652.6930\n",
            "Epoch 00085: loss improved from 696.93262 to 652.05627, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 652.0563\n",
            "Epoch 86/100\n",
            "421/435 [============================>.] - ETA: 0s - loss: 665.6418\n",
            "Epoch 00086: loss did not improve from 652.05627\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 666.9675\n",
            "Epoch 87/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 693.1123\n",
            "Epoch 00087: loss did not improve from 652.05627\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 692.1105\n",
            "Epoch 88/100\n",
            "435/435 [==============================] - ETA: 0s - loss: 698.5544\n",
            "Epoch 00088: loss did not improve from 652.05627\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 698.5544\n",
            "Epoch 89/100\n",
            "423/435 [============================>.] - ETA: 0s - loss: 699.0633\n",
            "Epoch 00089: loss did not improve from 652.05627\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 700.7260\n",
            "Epoch 90/100\n",
            "422/435 [============================>.] - ETA: 0s - loss: 595.5428\n",
            "Epoch 00090: loss improved from 652.05627 to 597.78217, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 597.7822\n",
            "Epoch 91/100\n",
            "430/435 [============================>.] - ETA: 0s - loss: 663.0872\n",
            "Epoch 00091: loss did not improve from 597.78217\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 674.0836\n",
            "Epoch 92/100\n",
            "420/435 [===========================>..] - ETA: 0s - loss: 659.0637\n",
            "Epoch 00092: loss did not improve from 597.78217\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 656.7300\n",
            "Epoch 93/100\n",
            "417/435 [===========================>..] - ETA: 0s - loss: 654.2433\n",
            "Epoch 00093: loss did not improve from 597.78217\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 653.3530\n",
            "Epoch 94/100\n",
            "427/435 [============================>.] - ETA: 0s - loss: 675.9515\n",
            "Epoch 00094: loss did not improve from 597.78217\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 673.8802\n",
            "Epoch 95/100\n",
            "428/435 [============================>.] - ETA: 0s - loss: 643.5723\n",
            "Epoch 00095: loss did not improve from 597.78217\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 644.8979\n",
            "Epoch 96/100\n",
            "429/435 [============================>.] - ETA: 0s - loss: 600.6686\n",
            "Epoch 00096: loss did not improve from 597.78217\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 600.8151\n",
            "Epoch 97/100\n",
            "423/435 [============================>.] - ETA: 0s - loss: 633.2336\n",
            "Epoch 00097: loss did not improve from 597.78217\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 633.4412\n",
            "Epoch 98/100\n",
            "433/435 [============================>.] - ETA: 0s - loss: 597.2311\n",
            "Epoch 00098: loss improved from 597.78217 to 596.72406, saving model to weight.bike.preprocess.best.hdf5\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 596.7241\n",
            "Epoch 99/100\n",
            "425/435 [============================>.] - ETA: 0s - loss: 626.7026\n",
            "Epoch 00099: loss did not improve from 596.72406\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 634.3276\n",
            "Epoch 100/100\n",
            "424/435 [============================>.] - ETA: 0s - loss: 637.2670\n",
            "Epoch 00100: loss did not improve from 596.72406\n",
            "435/435 [==============================] - 1s 3ms/step - loss: 639.9244\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f928001e3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrRlm2hMxek3",
        "colab_type": "text"
      },
      "source": [
        "## Validate the model using validation data by finding *RMSE*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYMCmS1V6rEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUAzgfVb7gVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNzxiseJ7ugx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "y_test = np.array(Y_test)    # Convert y_test into numpy array otherwise can not be compared with predicted output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncKKVYs6xtgf",
        "colab_type": "text"
      },
      "source": [
        "### RMSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAoMjeHU72jV",
        "colab_type": "code",
        "outputId": "e5361c97-9e28-4733-e937-5ca30673b3cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "RMSE = np.sqrt(mean_squared_error(y_test,pred))\n",
        "print('RMSE of the model\\n',RMSE)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE of the model\n",
            " 40.71235987423827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euvViFCW8Dvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}